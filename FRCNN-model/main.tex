\documentclass[11pt,addpoints,answers]{exam}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsfonts}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{titling}
\usepackage{url}
\usepackage{xfrac}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{paralist}
\usepackage{epstopdf}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}
\usepackage{fancyvrb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}
\usepackage{paralist}
\usepackage[svgname]{xcolor}
\usepackage{enumerate}
\usepackage{array}
\usepackage{times}
\usepackage{url}
\usepackage{comment}
\usepackage{environ}
\usepackage{times}
\usepackage{textcomp}
\usepackage{caption}
\usepackage[colorlinks=true,urlcolor=blue]{hyperref}
\usepackage{listings}
\usepackage{parskip} % For NIPS style paragraphs.
\usepackage[compact]{titlesec} % Less whitespace around titles
\usepackage[inline]{enumitem} % For inline enumerate* and itemize*
\usepackage{datetime}
\usepackage{comment}
% \usepackage{minted}
\usepackage{lastpage}
\usepackage{color}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{shapes,decorations,bayesnet}
%\usepackage{framed}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{cprotect}
\usepackage{xcolor}
\usepackage{verbatimbox}
\usepackage[many]{tcolorbox}
\usepackage{cancel}
\usepackage{wasysym}
\usepackage{mdframed}
\usepackage{subcaption}
\usetikzlibrary{shapes.geometric}

\newtcbtheorem[]{solution}{Solution}{breakable, fonttitle=\bfseries}{x}

\begin{document}
\begin{center}
    \textsc{\LARGE HW2-18-794: Introduction to Deep Learning and Pattern Recognition for Computer Vision} \\
    \vspace{1em}
    \textsc{\large Assignment 2: Face Detection} \\
    \vspace{0.5em}
    \textsc{\large Instructor: Marios Savvides} \\
    \vspace{0.5em}
    \textbf{Due Date:} Oct 19, 2023 \\
    \vspace{0.5em}
    \textbf{Total Points:} 100 \\
    \vspace{0.5em}
    \textbf{Submission:} Submit your solutions and plots on Gradescope.
\end{center}

\section*{START HERE: Instructions}
\begin{itemize}
\item \textbf{Collaboration policy:} All are encouraged to work together BUT you must do your own work (code and write up). If you work with someone, please include their name in your write-up and cite any code that has been discussed. If we find highly identical write-ups or code or lack of proper accreditation of collaborators, we will take action according to strict university policies. See the \href{hhttps://www.cmu.edu/policies/student-and-student-life/academic-integrity.html}{Academic Integrity Section} detailed in the initial lecture for more information.

\item\textbf{Late Submission Policy:} There are a \textbf{total of 5} late days across all homework submissions. Submissions more than 5 days after the deadline will receive a 0.

\textbf{For those students taking Mini-1 (semi-semester), you cannot submit it later than Oct 21, 2023, as the grading deadline of mini-1 is due on Oct 23, 2023.}

\item\textbf{Submitting your work:}

\begin{itemize}

\item We will be using Gradescope (\url{https://gradescope.com/}) to submit the Problem Sets. Please use the provided template only. Submissions must be written in LaTeX. All submissions not adhering to the template will not be graded and receive a zero. 
\item \textbf{Deliverables:} Please submit all the \texttt{.py} files. Add all relevant plots and text answers in the boxes provided in this file. TO include plots you can simply modify the already provided latex code. Submit the compiled \texttt{.pdf} report as well.
\end{itemize}
\end{itemize}
\emph{NOTE: Partial points will be given for implementing parts of the homework even if you don't get the mentioned numbers as long as you include partial results in this pdf.}
\clearpage

\section*{Overview}
In this assignment, you will train and test a state-of-the-art face detector that is deployed in the real world applications.

\section{Understand your data (POINTS : 30)}
Understanding your data is always the first step in training deep learning models, which
will greatly ease the following works. This step usually involves statistical analysis,
preprocessing, and visualization for detection datasets.

Data path: 

\url{https://drive.google.com/file/d/1799456S54_M7CS9Gom1xrD9_tIKavPuB/view?usp=sharing}

Note 1. The bounding box is indicated by the first 4 floats and is in (x1, y1, w, h). Where
x1, y1 is the top-left corner, followed by the 5 landmarks points ($L_x$, $L_y$), each landmark point 
 is followed by a 0/1 that you can ignore. The landmarks
might be -1 indicating no landmark available for this face.


\subsection{Write code to do the following statistical analysis}

1. Total number of ground truths (GT), i.e., faces, for training and validation.


2. Average area, width, and height of bounding boxes


3. Are faces uniformly distributed on images? Give your analysis.


4. Check if any GT exceeds the range of the image. If yes, how do you plan to solve it?


5. How serious is the mutual overlap among GTs? Calculate the percentage of GTs that overlap with one another (i.e., overlapped GT vs total number
of GT). Hint: check the overlap using IOU $>$ 0.

\begin{solution*}{}
Enter your solution here

\end{solution*}

\subsection{Visualize the first 4 images and their ground truths, including the faces (in
green) and landmarks (in red).}


\begin{solution*}{}
Enter your solution here

\end{solution*}

\section{Initialize your detector and build up the forward path (./detector/mydetector.py).}

This face detector has 4 components: 1. ResNet 50 as Backbone. 2. Feature pyramid
network (FPN). 3. A special module named SSH. 4. Three heads for classification,
bounding box regression, and landmarks, respectively.

Note 1: Since the FPN needs a feature pyramid extracted from the backbone, you will need to use 
\begin{verbatim}
  torchvision.models._utils.IntermediateLayerGetter
\end{verbatim}

to gather these intermediate layers from backbone.

Note 2: The classification head aims to classify each anchor as positive or negative, because we
are doing face/not face here. Regression aims to regress a bounding box for each
anchor, it requires 4 degrees of freedom to do so. Landmark head aims to regress 5
landmark points, it requires 10 degrees of freedom to do so.

\subsection{Initialize backbone and gather intermediate layers, the layers to-be-gather are
defined in cfg['return layers']}

Note: cfg is "configuration" that is specified in config.py

\begin{solution*}{}
backbone = ~~~ \#  initialize backbone

self.body = ~~~ \# gather intermediate layers from backbone
\end{solution*}

\subsection{Understand how we make the classification head. What is the output dimension
of the head?  And, for the binary classification, is sigmoid mathematically equivalent to softmax?  Show your proof.}

\begin{solution*}{}

\end{solution*}

\subsection{Following the code of the classification head, finish the regression head and
landmarks head.}

\begin{solution*}{}

\begin{verbatim}
class BboxHead(nn.Module):
    def __init__(self, inchannels=512, num_anchors=3):
        super(BboxHead,self).__init__()
        # TODO
        self.conv1x1 =

\end{verbatim}

\begin{verbatim}
class LandmarkHead(nn.Module):
    def __init__(self, inchannels=512, num_anchors=3):
        super(LandmarkHead, self).__init__()
        # TODO
        self.conv1x1 =
\end{verbatim}
        
\end{solution*}

\subsection{Complete the forward function of your detection. The process is Input image
 $->$ gather feature maps  $->$ FPN $->$you got 3 feature pyramids  $->$ for each feature
pyramid, apply a predefined SSH module to it  $->$for each feature pyramid, apply
three heads to it.}


\section{Dataloader (data/dataloader.py)}
Your dataloader feeds images, bounding boxes, and landmarks to the detector for
training.

\subsection{initialize dataloader}

\begin{verbatim}
    In __init__, load the widerface/train/label.txt, 
    and store the path of images in self.imgs_path as a list. 
    Store the annotations in self.words. The structure must be
    self.imgs_path: [path1, path2, path3]
    self.words: [[img1_anno1, img1_anno2, ...]
                [img2_anno1, img2_anno2, img2_anno3, ...]
                [img3_anno1, ...]
                    ...]
    Make sure you are storing numbers in float.
\end{verbatim}


\subsection{getitem}

Getitem is a commonly seen function in pytorch based dataloader. It aims to get/organize/process the training data from the pool prepared for each iteration.  

Read carefully:

\begin{verbatim}
    In __getitem__, complete the annotation reorganization. Each "annotation"
    should be in the following format:
    np.array(x1, y1, x2, y2, L0_x, L0_y, L1_x, L1_y, ..., L4_x, L4_y, M)
    Where (x2, y2) is the bottom-right corner of the bounding box, 
    L stands for the landmark points.
    M is an indicator showing whether landmark points are
    available, 
    Yes for 1 and No for -1. The "annotation" is in shape (1, 15)
    
\end{verbatim}


\section{Anchor(anchor.py)}
Anchor has been an important concept for detection since 2015 (Faster RCNN). 

We define "Anchor", in this homework, as "all reference boxes located in every feature point"

Understand the
code, and answer the following

\subsection{Name all hyperparameters that correspond to the number of anchors. I.e., which hyperparameters in config will affect the number of anchors?  Briefly describe their physical meanings in your words}
\begin{solution*}{}

\end{solution*}
\subsection{If we enlarge the width and height of the image by x, how would the number of anchors change?}
\begin{solution*}{}

\end{solution*}
\subsection{For anchors on different feature pyramids, are they the same size? Explain your
observations and explain why it is designed in such a way.}

\begin{solution*}{}

\end{solution*}


\section{Calculate the Loss}

Anchor plays a key role in anchor-based object detection for deciding which feature point
on the feature pyramid should be responsible for learning a certain ground-truth. Let’s do
a quick review:

On your multi-level feature pyramid, each pixel is a feature point. Each feature point is
mapped to one or multiple anchors depending on the detector’s setting. We use an anchor to
compare with ground-truth via “jaccard index”, i.e., Intersection over Union. If the IoU is larger
than a threshold (0.2), then the feature point corresponds to that anchor would be assign to
learn the ground-truth, the learning includes positive/negative classification, bounding box
regression, and landmark. If an anchor matches zero ground-truth, the corresponding feature
point needs to be classified as negative.

\subsection{In utils/boxutils.py, complete jaccard function.}

\subsection{Explain the following code in your words:}

In “match”, what is the following code used for?
\begin{verbatim}
best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)
\end{verbatim}

\begin{solution*}{}

\end{solution*}

In “match”, what is the following code used for?

\begin{verbatim}
valid_gt_idx = best_prior_overlap[:, 0] >= 0.2
\end{verbatim}

\begin{solution*}{}

\end{solution*}


In “match”, what is the following code used for?

\begin{verbatim}
best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)
\end{verbatim}
\begin{solution*}{}

\end{solution*}


\section{NMS}
Non-maximum suppression is a must-ask question for those interviews seeking computer vision engineer positions. Prior to 2020, it is an essential component for object detection and many other
tasks. Write function of NMS in nms.py

\begin{verbatim}
input: dets: ndarray (nx5), where n is number of predictions, 
and each prediction is in (x1, y1, x2, u2, confidence)
        thres: float, which is IOU threshold.
output: list, the index of the predictions in dets, which passed NMS. 
\end{verbatim}


\section{Training}
Train your detector with 1 epoch, and plot each loss curve, the plotting step is per 20 iterations.

\begin{solution*}{}

\end{solution*}

Use any methods you feel comfortable with to remove the landmark supervision. Explain your method.  And what do you expect the face detector would perform (better or worse) after the removal? Explain your answer.
\begin{solution*}{}

\end{solution*}

\section{Testing}
Load the checkpoint we provided, it is the same as yours but trained with larger batchsize and with 180 epochs. Plot the first 4 results of your detector, with confidence,
face bounding box, and landmarks.

\begin{solution*}{}

\end{solution*}


Enjoy your face detector and feel free to deploy it for your home security.





\end{document}